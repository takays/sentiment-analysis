{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtR0YZ8xDKGxtBgYB/9pBr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takays/sentiment-analysis/blob/main/sentiment_dic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 辞書単語のスコアをもとに文章のポジネガを計算するプログラム(Sudachi使用)"
      ],
      "metadata": {
        "id": "pQIgcy8Dlq7m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXMbAxN0BNVf"
      },
      "source": [
        "#### Google Driveマウント & データフォルダ設定 & ライブラリ設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAfs6WJxA30u"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sudachipy sudachidict_core"
      ],
      "metadata": {
        "id": "Ct2m0EgXSpz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUBmP345QNge"
      },
      "source": [
        "#### テキスト分析"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sudachipy import tokenizer, dictionary\n",
        "\n",
        "# Sudachiの初期設定\n",
        "tokenizer_obj = dictionary.Dictionary().create()\n",
        "mode = tokenizer.Tokenizer.SplitMode.C\n",
        "MAX_TEXT_LENGTH = 10000  # 適切な分割サイズを設定\n",
        "\n",
        "# 単語リストを読み込む関数\n",
        "def load_word_list(word_list_csv):\n",
        "    df = pd.read_csv(word_list_csv)\n",
        "    word_list = df[['word', 'score']].set_index('word').to_dict()['score']\n",
        "    return word_list\n",
        "\n",
        "# テキストの前処理関数\n",
        "def preprocess_text(text):\n",
        "    # テキストの先頭の一文字を削除\n",
        "    text = text[1:]\n",
        "    # 【】で囲まれた部分を削除\n",
        "    text = re.sub(r'【.*?】', '', text)\n",
        "    return text\n",
        "\n",
        "# テキストを最大長に分割\n",
        "def chunk_text(text, max_length):\n",
        "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "# Sudachiを使用してテキストを形態素解析し、単語の出現回数をカウント\n",
        "def analyze_text(text, word_list):\n",
        "    word_count = {}\n",
        "    chunks = chunk_text(text, MAX_TEXT_LENGTH)\n",
        "    for chunk in chunks:\n",
        "        tokens = tokenizer_obj.tokenize(chunk, mode)\n",
        "        for token in tokens:\n",
        "            word = token.surface()\n",
        "            if word in word_list:\n",
        "                if word in word_count:\n",
        "                    word_count[word] += 1\n",
        "                else:\n",
        "                    word_count[word] = 1\n",
        "    return word_count\n",
        "\n",
        "def analyze_line_by_line(text, word_list):\n",
        "    word_count = {}\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines:\n",
        "        line = preprocess_text(line)\n",
        "        #print(line)\n",
        "        line_word_count = analyze_text(line, word_list)\n",
        "        for word, count in line_word_count.items():\n",
        "            if word in word_count:\n",
        "                word_count[word] += count\n",
        "            else:\n",
        "                word_count[word] = count\n",
        "    return word_count\n",
        "\n",
        "# カウントされた単語の出現回数に基づいて、正の単語数とスコア、負の単語数とスコアを計算\n",
        "def calculate_scores(word_count, word_list):\n",
        "    pos_count = neg_count = pos_score = neg_score = 0\n",
        "    for word, count in word_count.items():\n",
        "        score = word_list[word] * count\n",
        "        if score > 0:\n",
        "            pos_count += count\n",
        "            pos_score += score\n",
        "        elif score < 0:\n",
        "            neg_count += count\n",
        "            neg_score += score\n",
        "    total_score = pos_score + neg_score\n",
        "    return pos_count, pos_score, neg_count, neg_score, total_score\n",
        "\n",
        "def main(word_list_csv, text_csv, output_csv):\n",
        "    word_list = load_word_list(word_list_csv)\n",
        "    df = pd.read_csv(text_csv)\n",
        "\n",
        "    results = []\n",
        "    for i, row in enumerate(df.iterrows()):\n",
        "        row_data = row[1]\n",
        "        text = row_data[5]\n",
        "        word_count = analyze_line_by_line(text, word_list)\n",
        "        pos_count, pos_score, neg_count, neg_score, total_score = calculate_scores(word_count, word_list)\n",
        "        results.append([\n",
        "            row_data[0], row_data[1], row_data[2], row_data[3], row_data[4], row_data[6], row_data[7],\n",
        "            pos_count, pos_score, neg_count, neg_score, total_score\n",
        "        ])\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Processing row {i}\")\n",
        "            #break\n",
        "\n",
        "    result_df = pd.DataFrame(results, columns=[\n",
        "        'name', 'ticker', 'nkcode', '報告日', '決算日', 'year', 'month',\n",
        "        '正の単語数', '正のスコア合計', '負の単語数', '負のスコア合計', 'スコア合計'\n",
        "    ])\n",
        "    result_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Results saved to {output_csv}\")"
      ],
      "metadata": {
        "id": "bPSwS3Zqd8Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 実行"
      ],
      "metadata": {
        "id": "tbTojZvizth2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルのパスを指定\n",
        "word_list_csv = 'polarity_dic_News.csv'  # 単語リストとスコアを含むCSVファイル\n",
        "text_csv = 'MDA_DataSet_2014_2022_TSE1.csv'  # テキストを含むCSVファイル\n",
        "output_csv = 'MDA_DataSet_2014_2022_TSE1_dic.csv'  # 結果を保存するCSVファイル\n",
        "\n",
        "# メイン処理を実行\n",
        "main(word_list_csv, text_csv, output_csv)"
      ],
      "metadata": {
        "id": "dmyyfd0CTyYr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}